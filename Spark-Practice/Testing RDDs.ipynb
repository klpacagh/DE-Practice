{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"TestingRDDs\").getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = \"Spark makes life a lot easier and puts me into good Spirits, Spark is too Awesome!\".split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Spark', 'makes', 'life', 'a', 'lot', 'easier', 'and', 'puts', 'me', 'into', 'good', 'Spirits,', 'Spark', 'is', 'too', 'Awesome!']\n"
     ]
    }
   ],
   "source": [
    "print(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load into RDD\n",
    "words_rdd = spark.sparkContext.parallelize(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to retrieve values in the rdd, we need to collect into a list\n",
    "words_data = words_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark\n",
      "makes\n",
      "life\n",
      "a\n",
      "lot\n",
      "easier\n",
      "and\n",
      "puts\n",
      "me\n",
      "into\n",
      "good\n",
      "Spirits,\n",
      "Spark\n",
      "is\n",
      "too\n",
      "Awesome!\n"
     ]
    }
   ],
   "source": [
    "for word in words_data:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 \n",
      "\n",
      "Current RDD: \n",
      "Spark\n",
      "makes\n",
      "life\n",
      "a\n",
      "lot\n",
      "easier\n",
      "and\n",
      "puts\n",
      "me\n",
      "into\n",
      "good\n",
      "Spirits,\n",
      "Spark\n",
      "is\n",
      "too\n",
      "Awesome!\n"
     ]
    }
   ],
   "source": [
    "# Distinct Transformation\n",
    "# Does not alter the RDD!\n",
    "print(words_rdd.distinct().count(), \"\\n\")\n",
    "words_data = words_rdd.collect()\n",
    "\n",
    "print(\"Current RDD: \")\n",
    "for word in words_data:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New RDD: \n",
      "life\n",
      "and\n",
      "Awesome!\n",
      "Spirits,\n",
      "good\n",
      "makes\n",
      "a\n",
      "Spark\n",
      "into\n",
      "easier\n",
      "too\n",
      "lot\n",
      "puts\n",
      "is\n",
      "me\n"
     ]
    }
   ],
   "source": [
    "# Need to assign to new RDD\n",
    "words_unique_rdd = words_rdd.distinct()\n",
    "\n",
    "print(\"New RDD: \")\n",
    "for word in words_unique_rdd.collect():\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordStartsWith(word, letter):\n",
    "    return word.startswith(letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark', 'Spirits,', 'Spark']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_rdd.filter(lambda word: wordStartsWith(word, \"S\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting Transformations\n",
    "countries_list = [(\"India\",91),(\"USA\",4),(\"Greece\",13)]\n",
    "countries_rdd = spark.sparkContext.parallelize(countries_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_countries_list = countries_rdd.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Greece', 13)\n",
      "('India', 91)\n",
      "('USA', 4)\n"
     ]
    }
   ],
   "source": [
    "for country in sorted_countries_list:\n",
    "    print(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 'India')\n",
      "(13, 'Greece')\n",
      "(4, 'USA')\n"
     ]
    }
   ],
   "source": [
    "# Map Transformation\n",
    "# the c represents each tuple, we want to switch the key and value\n",
    "sorted_countries_list = countries_rdd.map(lambda c: (c[1],c[0])\n",
    "                                         ).sortByKey(False).collect()\n",
    "\n",
    "for country in sorted_countries_list:\n",
    "    print(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# Reduce Action, summation example\n",
    "num_list = [1,5,2,3,4]\n",
    "result = spark.sparkContext.parallelize(num_list).reduce(lambda x, y: x + y)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumList(x, y): #print x and y as they enter the function\n",
    "    print(x,y)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 5\n",
      "6 2\n",
      "8 3\n",
      "11 4\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "result = spark.sparkContext.parallelize(num_list).reduce(lambda x, y: sumList(x,y))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using reducer to get the largest length word in the list\n",
    "def wordLengthReducer(leftWord, rightWord):\n",
    "    print(leftWord, \" compared to \", rightWord)\n",
    "    if len(leftWord) > len(rightWord):\n",
    "        return leftWord\n",
    "    else:\n",
    "        return rightWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark  compared to  makes\n",
      "makes  compared to  life\n",
      "makes  compared to  lot\n",
      "makes  compared to  easier\n",
      "easier  compared to  puts\n",
      "easier  compared to  me\n",
      "easier  compared to  into\n",
      "easier  compared to  Spirits,\n",
      "Spirits,  compared to  Spark\n",
      "Spirits,  compared to  is\n",
      "Spirits,  compared to  Awesome!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Awesome!'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_rdd.reduce(wordLengthReducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spark'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize(range(1,21)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize(range(1,21)).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
